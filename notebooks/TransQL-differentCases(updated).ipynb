{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "import MeCab\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import xlrd\n",
    "import string\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "mecab = MeCab.Tagger(\"-Owakati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words in the vocabulary = 2218\n"
     ]
    }
   ],
   "source": [
    "# Select all words in the data file and compute the vocabulary. \n",
    "# Write the cross-lingual word embeddings for those words to a separate file.\n",
    "# This will speed up loading word embeddings and save memory.\n",
    "\n",
    "data_files = [\"../data/newdata.xlsx\", \"../data/olddata.xlsx\", \"../data/google_translation.xlsx\"]\n",
    "#data_files = [\"../data/newdata.xlsx\", \"../data/olddata.xlsx\"]\n",
    "vocab = set()\n",
    "for fname in data_files:\n",
    "    trans_data = xlrd.open_workbook(fname)\n",
    "    sheet = trans_data.sheet_by_index(0)  \n",
    "    for l in range(1, sheet.nrows):\n",
    "        # tokenise Japanese texts\n",
    "        rows = sheet.row_values(l, 0, sheet.ncols)\n",
    "        token_ja = mecab.parse(rows[0].lower())\n",
    "        vocab = vocab.union(set(token_ja.strip().split()))    \n",
    "        # tokenise English texts\n",
    "        vocab = vocab.union(set(nltk.word_tokenize(rows[1].lower())))\n",
    "\n",
    "stop_words = ['(', ')', '[', ']', '@', '•', '`', '-', '❚❚', '●', '（√',  '×', '。', '＠']\n",
    "add_words = ['I', 'like', 'hate', 'cat', 'cats', 'dog', 'dogs', 'banana', '好き', '嫌い', '猫', '犬', '私']\n",
    "vocab = vocab - set(stop_words)\n",
    "vocab = vocab.union(set(add_words))\n",
    "print(\"No of unique words in the vocabulary = %d\" % len(vocab))\n",
    "\n",
    "# write the vocabulary to a file for debugging purposes\n",
    "with open(\"../data/vocab.txt\", 'w') as vocab_file:\n",
    "    for word in vocab:\n",
    "        vocab_file.write(\"%s\\n\" % word)\n",
    "\n",
    "# Lets select the cross-lingual word embeddings for those words in the vocabulary.\n",
    "cross_in_embeds_fname = \"../data/ja2en.txt\"\n",
    "cross_out_embeds_fname = \"../data/ja2en.sel\"\n",
    "first_line = True\n",
    "\n",
    "with open(cross_in_embeds_fname) as cross_in:\n",
    "    with open(cross_out_embeds_fname, 'w') as cross_out:\n",
    "        for line in cross_in:\n",
    "            if first_line:\n",
    "                dim = int(line.split()[1])\n",
    "                cross_out.write(\"%d %d\\n\" % (len(vocab), dim))\n",
    "                first_line = False\n",
    "            elif line.split()[0].lower() in vocab:\n",
    "                cross_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cross-lingual word embeddings.\n",
    "large_embeddings = gensim.models.KeyedVectors.load_word2vec_format('../data/ja2en.txt')\n",
    "small_embeddings = gensim.models.KeyedVectors.load_word2vec_format('../data/ja2en.sel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = large_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    stop_words = ['(', ')', '[', ']', '@', '•', '`', '-', '❚❚', '●', '（√',  '×', '。', '＠']\n",
    "    for ch in stop_words:\n",
    "        s = s.replace(ch, ' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def av(source, target):\n",
    "    # remove words that are not in the vocabulary from source and target.\n",
    "    source = list(filter(lambda x: x in embeddings, source))\n",
    "    target = list(filter(lambda x: x in embeddings, target))\n",
    "    similarity_average = []\n",
    "    first = []\n",
    "    second = []\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    for i in range(n):\n",
    "        first.append(embeddings[source[i]])\n",
    "    for j in range(m):\n",
    "        second.append(embeddings[target[j]])\n",
    "    source_average = np.mean(first, axis=0)\n",
    "    target_average = np.mean(second, axis=0) \n",
    "    #similarity_average = scipy.spatial.distance.cosine(source_average, target_average)\n",
    "    similarity_average = np.dot(source_average, target_average)/(np.linalg.norm(source_average)*np.linalg.norm(target_average))\n",
    "    return similarity_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sms(source, target):\n",
    "    source = list(filter(lambda x: x in embeddings, source))\n",
    "    target = list(filter(lambda x: x in embeddings, target))\n",
    "    sim_max = []\n",
    "    \n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    \n",
    "    for i in range(n):\n",
    "        temp_max = 0\n",
    "        for j in range(m):\n",
    "            first, second = embeddings[source[i]],  embeddings[target[j]]   \n",
    "            #similarity_temp = scipy.spatial.distance.cosine(first, second)\n",
    "            \n",
    "            similarity_temp = np.dot(first, second)/(np.linalg.norm(first)*(np.linalg.norm(second)))\n",
    "            #print(\"sim_temp\", similarity_temp)\n",
    "            if temp_max < similarity_temp:\n",
    "                temp_max = similarity_temp\n",
    "        sim_max.append(temp_max) \n",
    "    #sim_max = np.array(sim_max)        \n",
    "    #print(\"sim_max\", sim_max)\n",
    "    similarity = np.mean(sim_max, axis=0) \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmd(source, target):\n",
    "    distance = embeddings.wmdistance(source, target)\n",
    "    return (distance, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwmd(source, target):\n",
    "    # remove words that are not in the vocabulary from source and target.\n",
    "    source = list(filter(lambda x: x in embeddings, source))\n",
    "    target = list(filter(lambda x: x in embeddings, target))\n",
    "     \n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    \n",
    "    # compute distances between words\n",
    "    C = np.zeros((n, m), dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            first, second = embeddings[source[i]],  embeddings[target[j]]\n",
    "            \n",
    "#             # normalisation\n",
    "#             first_norm, second_norm = np.linalg.norm(first, ord=1), np.linalg.norm(second, ord=1)\n",
    "#             if first_norm > 0:\n",
    "#                 first = first / first_norm\n",
    "#             if second_norm > 0:\n",
    "#                 second = second / second_norm\n",
    "            \n",
    "            C[i,j] = scipy.spatial.distance.euclidean(first, second)\n",
    "    \n",
    "    # Initialise variables\n",
    "    x = np.zeros(n + n*m, dtype=float)\n",
    "    T = x[n:].reshape(n,m)\n",
    "    y = x[:n]\n",
    "    \n",
    "    c = np.zeros_like(x)\n",
    "    c[:n] = 1.0\n",
    "    \n",
    "    # Inequality constraints\n",
    "    b_ub = np.zeros(n*m, dtype=float)\n",
    "    A_ub = np.zeros((n*m, n + n*m), dtype=float)    \n",
    "    for p in range(n*m):\n",
    "        for q in range(n + n*m):\n",
    "            if p % n == q:\n",
    "                A_ub[p, q % n] = -1.0\n",
    "            if (p // n) + 2 * (p % n) + n == q:\n",
    "                A_ub[p,q] = C[p % n, p // n]    \n",
    "    #print(A_ub)\n",
    "    \n",
    "    # Equality constraints for Eq. 5 (Columns in T must be stochastic)\n",
    "    CA_eq = np.zeros((n, n + n*m), dtype=float)\n",
    "    Cb_eq = np.ones(n, dtype=float)\n",
    "    for p in range(n):\n",
    "        for q in range(n + m*p, n + m + m*p):\n",
    "            CA_eq[p,q] = 1.0\n",
    "            \n",
    "    # Equality constraints for Eq. 4 (Rows in T must be stochastic)\n",
    "    RA_eq = np.zeros((m, n + n*m), dtype=float)\n",
    "    Rb_eq = np.ones(m, dtype=float)\n",
    "    for p in range(m):\n",
    "        for q in range(n, n + n*m):\n",
    "            if p == (q - n) % m:\n",
    "                RA_eq[p,q] = 1.0\n",
    "    \n",
    "    # Double stochasticity\n",
    "    #A_eq = np.concatenate((CA_eq, RA_eq), axis=0)\n",
    "    #b_eq = np.concatenate((Cb_eq, Rb_eq), axis=0)    \n",
    "    \n",
    "    res = scipy.optimize.linprog(c, A_ub, b_ub, CA_eq, Cb_eq, method='interior-point', options={'maxiter':10000})\n",
    "    #res = scipy.optimize.linprog(c, A_ub, b_ub, method='simplex') \n",
    "    status = {0 : \"Optimization terminated successfully\",\n",
    "              1 : \"Iteration limit reached\",\n",
    "              2 : \"Problem appears to be infeasible\",\n",
    "              3 : \"Problem appears to be unbounded\",\n",
    "              4 : \"Serious numerical difficulties encountered\"}\n",
    "    if res.status > 0:\n",
    "        print(\"\\x1b[31m %s \\x1b[0m\" % status[res.status])\n",
    "    \n",
    "    if res.status == 2:\n",
    "        # Infeasible problem. Drop equality constrains and try again.\n",
    "        res = scipy.optimize.linprog(c, A_ub, b_ub, method='interior-point') \n",
    "        distance_y = np.sum(res.x[:n])\n",
    "        distance_TC = C.flatten().dot(res.x[n:])\n",
    "        return (distance_TC, 2)        \n",
    "    \n",
    "    if res.status == 0:        \n",
    "        print(\"No of iterations to optimisation = %d\" % res.nit)\n",
    "        # objective is the sum of y_i.\n",
    "        distance_y = np.sum(res.x[:n])\n",
    "        #print(\"sum y = \", distance_y)\n",
    "        distance_TC = C.flatten().dot(res.x[n:])\n",
    "        #print(\"sum TC = %f\" % distance_TC)\n",
    "        return (distance_TC, res.status)\n",
    "    else:\n",
    "        return (0, res.status) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances = 30\n",
      "[('各部名称。（ブレ軽減）ボタン。カッコ内の用語はアイコンを説明している。', 'Camera part; the “anti-shake” button. The term in parentheses is represented by an icon in the manuals except when it appears in the list of camera parts.', 0.30000000000000004), ('各部名称。メニュー操作時では、設定を決定するときに押す。カッコ内の用語はアイコンを説明している。', 'Camera part. Pressing this button applies a selection. The term in parentheses is represented by an icon in the manuals except when it appears in the list of camera parts.', 0.475), ('各部名称。ボタンを押すと表示中の画像を削除する。カッコ内の用語はアイコンを説明している。', 'Camera part. Pressing this button deletes the current picture. The term in parentheses is represented by an icon in the manuals except when it appears in the list of camera parts.', 0.5375)]\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 12\n",
      "No of iterations to optimisation = 12\n",
      "No of iterations to optimisation = 12\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 13\n",
      "No of iterations to optimisation = 13\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 13\n",
      "No of iterations to optimisation = 12\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 12\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 12\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 7\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 7\n",
      "No of iterations to optimisation = 7\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 11\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 9\n",
      "No of iterations to optimisation = 10\n",
      "No of iterations to optimisation = 15\n",
      "No of iterations to optimisation = 11\n",
      "StoT [ 67.99691383  85.81232287  75.55824314 109.40004135  42.45801504\n",
      "  39.42660878  71.62435459  71.82651815  94.10905407  86.20111202\n",
      "  97.54063957  85.6368605   88.89911559  94.14644107  95.41091245\n",
      "  92.25769686  48.98590825  43.81024252  58.3404787   74.30707831\n",
      "  77.09287849  85.55511352  86.87014932  79.51078119  31.78408269\n",
      "  17.99074123  64.42330946 137.12204356  47.45281462 138.64068761]\n",
      "Source to target failed cases = 0\n",
      "TtoS [ 67.65729641  65.2398602   65.37055503  76.26712434  77.84547061\n",
      "  72.40552433  71.7593228   68.56481306  74.09440807  63.00402251\n",
      "  94.1691486   72.54250355  88.92174173  87.24735768  75.06170707\n",
      "  79.11603277  74.79655036  80.96346987  78.85105971  81.01868515\n",
      " 110.60799294  53.45950065  44.94639877  37.06916594  63.70051708\n",
      "  20.78081662  35.37941279  52.75052108  46.86392298  66.20353886]\n",
      "Target to source failed cases = 0\n",
      "original distances [135.65421024 151.05218307 140.92879817 185.66716569 120.30348564\n",
      " 111.83213311 143.38367739 140.39133121 168.20346214 149.20513453\n",
      " 191.70978817 158.17936406 177.82085732 181.39379875 170.47261952\n",
      " 171.37372963 123.78245862 124.77371239 137.19153841 155.32576346\n",
      " 187.70087143 139.01461417 131.81654809 116.57994713  95.48459977\n",
      "  38.77155784  99.80272225 189.87256464  94.3167376  204.84422647]\n",
      "(30,)\n",
      "[0.3    0.475  0.5375 0.5625 0.4625 0.3875 0.4125 0.4875 0.5125 0.5875\n",
      " 0.4625 0.5625 0.4375 0.5375 0.4875 0.575  0.475  0.1775 0.45   0.425\n",
      " 0.325  0.2275 0.33   0.45   0.225  0.7625 0.525  0.5375 0.7625 0.725 ]\n",
      "(30,)\n",
      "distances [0.33776894 0.26259975 0.31201967 0.09361778 0.41270746 0.45406256\n",
      " 0.30003554 0.31464346 0.17887135 0.2716166  0.06411915 0.22780658\n",
      " 0.13192156 0.11447932 0.16779388 0.16339488 0.39572396 0.3908849\n",
      " 0.33026407 0.24173717 0.08368972 0.32136426 0.35650347 0.43088488\n",
      " 0.53386726 0.81072663 0.51278723 0.07308803 0.53956848 0.        ]\n",
      "(30,)\n",
      "Spearman Full SpearmanrResult(correlation=-0.20888552967083074, pvalue=0.2679595112594165)\n",
      "Pearson Full (-0.0016397461390015202, 0.9931385827851458)\n",
      "Sperman Low SpearmanrResult(correlation=-0.3148484465434674, pvalue=0.25304468800566526)\n",
      "Sperman High SpearmanrResult(correlation=0.010772149064210824, pvalue=0.9696066852425835)\n",
      "Pearson Low (-0.3401141700051059, 0.2148378033757812)\n",
      "Pearson High (0.38815299610646614, 0.1528102915805134)\n",
      "Accuracy =  36.666666666666664\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCQLA4OIiGRhYBARkSz/D3zH8Cp9M99OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29ba2eaa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will compute the correlation between human ratings and semantic distances over all instances\n",
    "                     \n",
    "trans_data = xlrd.open_workbook(\"../data/olddata.xlsx\")  #open the Excel spreadsheet as workbook\n",
    "sheet = trans_data.sheet_by_index(0)  \n",
    "instances = []\n",
    "for l in range(1, sheet.nrows):\n",
    "    ## tokenise Japanese texts\n",
    "    rows = sheet.row_values(l, 0, sheet.ncols)\n",
    "    instances.append((rows[0], rows[1], float(rows[2])))\n",
    "    \n",
    "print(\"Total number of instances = %d\" % len(instances))\n",
    "print(instances[:3])\n",
    "\n",
    "## 1000 random integers between 0 and 50\n",
    "human_ratings = []\n",
    "distancesst = []\n",
    "distancests = []\n",
    "bleuScores = []\n",
    "bad_count = 0\n",
    "\n",
    "# Evaluate the code in different configurations for mwmd method\n",
    "for x in instances:\n",
    "    source = list(set(mecab.parse(clean_text(x[0]).lower().strip('\\n')).split()))\n",
    "    target = list(set(nltk.word_tokenize(clean_text(x[1]).lower().strip())))\n",
    "    res = mwmd(source, target)\n",
    "    if res[1] > 0:\n",
    "        bad_count += 1\n",
    "    else:\n",
    "        distancesst.append(res[0])\n",
    "        \n",
    "        # Human ratings\n",
    "        human_ratings.append(x[2])\n",
    "        \n",
    "#         # BLEU scores\n",
    "#         bleuScores.append(x[2])\n",
    "        \n",
    "\n",
    "        \n",
    "    # Combine both source to target and target to source direction\n",
    "    # *************************************************************\n",
    "    target = list(set(mecab.parse(clean_text(x[0]).lower().strip('\\n')).split()))\n",
    "    source = list(set(nltk.word_tokenize(clean_text(x[1]).lower().strip())))\n",
    "    res1 = mwmd(source, target)\n",
    "    if res1[1] > 0:\n",
    "        bad_count += 1\n",
    "    else:\n",
    "        distancests.append(res1[0])\n",
    "        \n",
    "distancesst = np.array(distancesst)\n",
    "print(\"StoT\",distancesst)\n",
    "#print(\"distances\", distances)\n",
    "print(\"Source to target failed cases = %d\" % bad_count)\n",
    "\n",
    "distancests = np.array(distancests)\n",
    "print(\"TtoS\",distancests)\n",
    "print(\"Target to source failed cases = %d\" % bad_count)\n",
    "distances = distancesst + distancests\n",
    "# *************************************************************\n",
    "\n",
    "\n",
    "#distances = np.array(distancesst)   ## comment this line if using MWMD method and considering both directions\n",
    "\n",
    "\"\"\"\n",
    "# for average vector, SMS and TMS mehtods\n",
    "#-----------------------------------------\n",
    "for x in instances:\n",
    "    source = list(set(mecab.parse(clean_text(x[0]).lower().strip('\\n')).split()))\n",
    "    target = list(set(nltk.word_tokenize(clean_text(x[1]).lower().strip())))\n",
    "    res = av(source, target)\n",
    "    distancesst.append(res)\n",
    "    human_ratings.append(x[2])\n",
    "distances = np.array(distancesst)\n",
    "#-----------------------------------------\n",
    "\"\"\"\n",
    "print(\"original distances\", distances)\n",
    "print(distances.shape)\n",
    "\n",
    "\n",
    "## convert distances to similarity and scale to [0,1]\n",
    "human_ratings = np.array(human_ratings)\n",
    "#bleuScores = np.array(bleuScores)\n",
    "\n",
    "#--------------comment these two lines----------\n",
    "# human_ratings = 1.0 - (human_ratings / np.max(human_ratings))\n",
    "# human_ratings = human_ratings\n",
    "\n",
    "\n",
    "distances = np.array(distances)\n",
    "distances = 1.0 - (distances / np.max(distances))  # comment this line if using averagevector and sms\n",
    "\n",
    "print(human_ratings)\n",
    "print(human_ratings.shape)\n",
    "#print(bleuScores)\n",
    "# print(bleuScores.shape)\n",
    "print(\"distances\", distances)\n",
    "print(distances.shape)\n",
    "\n",
    "spr = scipy.stats.spearmanr(human_ratings, distances)\n",
    "pearson = scipy.stats.pearsonr(human_ratings, distances)\n",
    "print(\"Spearman Full\", spr)\n",
    "print(\"Pearson Full\", pearson)\n",
    "\n",
    "# Plot linear regression line\n",
    "fit = np.polyfit(human_ratings, distances, 1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(human_ratings, fit_fn(human_ratings), '--k')\n",
    "\n",
    "sortinds = np.argsort(human_ratings)\n",
    "distances = distances[sortinds]\n",
    "human_ratings = human_ratings[sortinds]\n",
    "N = len(sortinds) // 2\n",
    "low_bleu, high_bleu = human_ratings[: N], human_ratings[N:]\n",
    "low_sim, high_sim = distances[:N], distances[N:]\n",
    "print(\"Sperman Low\", scipy.stats.spearmanr(low_bleu, low_sim))\n",
    "print(\"Sperman High\", scipy.stats.spearmanr(high_bleu, high_sim))\n",
    "print(\"Pearson Low\", scipy.stats.pearsonr(low_bleu, low_sim))\n",
    "print(\"Pearson High\", scipy.stats.pearsonr(high_bleu, high_sim))\n",
    "\n",
    "# Compute accuracy. For low_human, predicted value must be less than or equal, \n",
    "# and for high_human predicted value must be greater than or equal to be correct.\n",
    "\n",
    "corrects = 0\n",
    "for (x,y) in zip(low_bleu, low_sim):\n",
    "    if fit_fn(x) >= y:\n",
    "        corrects += 1\n",
    "for (x,y) in zip(high_bleu, high_sim):\n",
    "    if fit_fn(x) <= y:\n",
    "        corrects += 1\n",
    "print(\"Accuracy = \", float(100 * corrects) / float(len(distances)))\n",
    "plt.plot(low_bleu, low_sim, 'b*', high_bleu, high_sim, 'r+')\n",
    "plt.xlabel(\"BLEU Scores\")\n",
    "plt.ylabel(\"Translation Quality\")\n",
    "plt.title(\"Spearman = %f, Pearson = %f\" % (spr[0], pearson[0]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1c3717b8e64a16b82be5d377dfe1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We provide a simple UI for entering source (Japanese) and target (English) texts to compare.\n",
    "\n",
    "def Comparison(Source_Ja, Target_En):\n",
    "    source = list(set(mecab.parse(Source_Ja.lower().strip('\\n')).split()))\n",
    "    target = list(set(nltk.word_tokenize(Target_En.lower().strip())))\n",
    "    print(source, target)\n",
    "    distance = mwmd(source, target)[0]\n",
    "    print(\"Semantic distance = %f\\n\" % distance)\n",
    "\n",
    "interact_manual(Comparison, Source_Ja='私は猫が好きです', Target_En=\"I like cats\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process a dataset, predict similarities and save to a file.\n",
    "trans_data = xlrd.open_workbook('../data/newdata.xlsx')  \n",
    "sheet = trans_data.sheet_by_index(0)  \n",
    "scores = []\n",
    "outputs = []\n",
    "distancesst = []\n",
    "distancests = []\n",
    "for l in range(1, sheet.nrows):\n",
    "    rows = sheet.row_values(l, 0, sheet.ncols)\n",
    "    source = list(set(mecab.parse(clean_text(rows[0]).lower().strip('\\n')).split()))\n",
    "    target = list(set(nltk.word_tokenize(clean_text(rows[1]).lower().strip())))\n",
    "    #res1 = sms(source, target)\n",
    "    #distancesst.append(res1)\n",
    "    res1 = mwmd(source, target)\n",
    "    vals_t = -1 if res1[1] > 0 else res1[0]\n",
    "    distancesst.append(vals_t)\n",
    "    \n",
    "    res2 = mwmd(target, source)\n",
    "    valt_s = -1 if res2[1] > 0 else res2[0]\n",
    "    distancests.append(valt_s)\n",
    "\n",
    "distancesst = np.array(distancesst)\n",
    "distancests = np.array(distancests)\n",
    "print(\"stot\",distancesst)\n",
    "print(\"ttos\",distancests)\n",
    "scores = distancesst + distancests\n",
    "#scores = distancesst\n",
    "\n",
    "\"\"\"\n",
    "    scores.append(val)\n",
    "    print(\"val\", val)\n",
    "    \n",
    "\"\"\"  \n",
    "np.savetxt(\"../data/distances.csv\", scores)\n",
    "#scores = np.array(scores)\n",
    "max_val = np.max(scores)\n",
    "print(\"max val\", max_val)\n",
    "scores = 1.0 - (scores / max_val)\n",
    "\"\"\"\n",
    "avg_val = np.average(scores)\n",
    "scores = (scores * 0.85) / avg_val\n",
    "\"\"\"\n",
    "with open(\"../data/pred-sims-newdata.csv\", \"w\") as out_file:\n",
    "    for val in scores:\n",
    "        #print(val)\n",
    "        out_file.write(\"%f\\n\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "-\tL2 normalised, l1 normalised vs. non-normalised (3 options)\n",
    "-\tY_sum vs TC_sum (2 options)\n",
    "-\tFull vocabulary vs. restricted vocabulary (2 options)\n",
    "-\tRow stochasticity, column stochasticity (2 options)\n",
    "\n",
    "- Do 3 x 2 x 2 x 2 = 24 experiments and produce the correlation plots. Prepare a table summarising the results (Spearman, Pearson for Full, High and Low, and accuracy)\n",
    "24 rows and 7 columns excel sheet!\n",
    "Decide which setting is the best.\n",
    "\n",
    "* Once the answer to this question is known, we will score the newdataset using wmd and the best version of the proposed method and get humans to judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
