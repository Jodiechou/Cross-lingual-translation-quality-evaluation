{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "import MeCab\n",
    "import nltk\n",
    "import xlrd\n",
    "import string\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "mecab = MeCab.Tagger(\"-Owakati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all words in the data file and compute the vocabulary. \n",
    "# Write the cross-lingual word embeddings for those words to a separate file.\n",
    "# This will speed up loading word embeddings and save memory.\n",
    "\n",
    "data_files = [\"../data/olddata.xlsx\", \"../data/newdata.xlsx\"]\n",
    "vocab = set()\n",
    "for fname in data_files:\n",
    "    trans_data = xlrd.open_workbook(fname)\n",
    "    sheet = trans_data.sheet_by_index(0)  \n",
    "    for l in range(1, sheet.nrows):\n",
    "        # tokenise Japanese texts\n",
    "        rows = sheet.row_values(l, 0, sheet.ncols)\n",
    "        token_ja = mecab.parse(rows[0].lower())\n",
    "        vocab = vocab.union(set(token_ja.strip().split()))    \n",
    "        # tokenise English texts\n",
    "        vocab = vocab.union(set(nltk.word_tokenize(rows[1].lower())))\n",
    "\n",
    "stop_words = ['(', ')', '[', ']', '@', '•', '`', '-', '❚❚', '●', '（√',  '×', '。', '＠']\n",
    "add_words = ['I', 'like', 'hate', 'cat', 'cats', 'dog', 'dogs', 'banana', '好き', '嫌い', '猫', '犬', '私']\n",
    "vocab = vocab - set(stop_words)\n",
    "vocab = vocab.union(set(add_words))\n",
    "print(\"No of unique words in the vocabulary = %d\" % len(vocab))\n",
    "\n",
    "# write the vocabulary to a file for debugging purposes\n",
    "with open(\"../data/vocab.txt\", 'w') as vocab_file:\n",
    "    for word in vocab:\n",
    "        vocab_file.write(\"%s\\n\" % word)\n",
    "\n",
    "# Lets select the cross-lingual word embeddings for those words in the vocabulary.\n",
    "cross_in_embeds_fname = \"../data/ja2en.txt\"\n",
    "cross_out_embeds_fname = \"../data/ja2en.sel\"\n",
    "first_line = True\n",
    "\n",
    "with open(cross_in_embeds_fname) as cross_in:\n",
    "    with open(cross_out_embeds_fname, 'w') as cross_out:\n",
    "        for line in cross_in:\n",
    "            if first_line:\n",
    "                dim = int(line.split()[1])\n",
    "                cross_out.write(\"%d %d\\n\" % (len(vocab), dim))\n",
    "                first_line = False\n",
    "            elif line.split()[0].lower() in vocab:\n",
    "                cross_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cross-lingual word embeddings.\n",
    "large_embeddings = gensim.models.KeyedVectors.load_word2vec_format('../data/ja2en.txt')\n",
    "small_embeddings = gensim.models.KeyedVectors.load_word2vec_format('../data/ja2en.sel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = large_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    stop_words = ['(', ')', '[', ']', '@', '•', '`', '-', '❚❚', '●', '（√',  '×', '。', '＠']\n",
    "    for ch in stop_words:\n",
    "        s = s.replace(ch, ' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwmd(source, target):\n",
    "    # remove words that are not in the vocabulary from source and target.\n",
    "    source = list(filter(lambda x: x in embeddings, source))\n",
    "    target = list(filter(lambda x: x in embeddings, target))\n",
    "     \n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    \n",
    "    # compute distances between words\n",
    "    C = np.zeros((n, m), dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            first, second = embeddings[source[i]],  embeddings[target[j]]\n",
    "            \n",
    "            first_norm, second_norm = np.linalg.norm(first), np.linalg.norm(second)\n",
    "            if first_norm > 0:\n",
    "                first = first / first_norm\n",
    "            if second_norm > 0:\n",
    "                second = second / second_norm\n",
    "            \n",
    "            C[i,j] = scipy.spatial.distance.euclidean(first, second)\n",
    "    \n",
    "    # Initialise variables\n",
    "    x = np.zeros(n + n*m, dtype=float)\n",
    "    T = x[n:].reshape(n,m)\n",
    "    y = x[:n]\n",
    "    \n",
    "    c = np.zeros_like(x)\n",
    "    c[:n] = 1.0\n",
    "    \n",
    "    # Inequality constraints\n",
    "    b_ub = np.zeros(n*m, dtype=float)\n",
    "    A_ub = np.zeros((n*m, n + n*m), dtype=float)    \n",
    "    for p in range(n*m):\n",
    "        for q in range(n + n*m):\n",
    "            if p % n == q:\n",
    "                A_ub[p, q % n] = -1.0\n",
    "            if (p // n) + 2 * (p % n) + n == q:\n",
    "                A_ub[p,q] = C[p % n, p // n]    \n",
    "    \n",
    "    # Equality constraints for Eq. 5 (Columns in T must be stochastic)\n",
    "    CA_eq = np.zeros((n, n + n*m), dtype=float)\n",
    "    Cb_eq = np.ones(n, dtype=float)\n",
    "    for p in range(n):\n",
    "        for q in range(n + m*p, n + m + m*p):\n",
    "            CA_eq[p,q] = 1.0\n",
    "            \n",
    "    # Equality constraints for Eq. 4 (Rows in T must be stochastic)\n",
    "    RA_eq = np.zeros((m, n + n*m), dtype=float)\n",
    "    Rb_eq = np.ones(m, dtype=float)\n",
    "    for p in range(m):\n",
    "        for q in range(n, n + n*m):\n",
    "            if p == (q - n) % m:\n",
    "                RA_eq[p,q] = 1.0\n",
    "    \n",
    "    \n",
    "    res = scipy.optimize.linprog(c, A_ub, b_ub, CA_eq, Cb_eq, method='interior-point', options={'maxiter':10000})\n",
    "    #res = scipy.optimize.linprog(c, A_ub, b_ub, method='simplex')\n",
    "    status = {0 : \"Optimization terminated successfully\",\n",
    "              1 : \"Iteration limit reached\",\n",
    "              2 : \"Problem appears to be infeasible\",\n",
    "              3 : \"Problem appears to be unbounded\",\n",
    "              4 : \"Serious numerical difficulties encountered\"}\n",
    "    if res.status > 0:\n",
    "        print(\"\\x1b[31m %s \\x1b[0m\" % status[res.status])\n",
    "    \n",
    "    if res.status == 2:\n",
    "        # Infeasible problem. Drop equality constrains and try again.\n",
    "        res = scipy.optimize.linprog(c, A_ub, b_ub, method='interior-point') \n",
    "        distance_y = np.sum(res.x[:n])\n",
    "        distance_TC = C.flatten().dot(res.x[n:])\n",
    "        return (distance_y, 2)        \n",
    "    \n",
    "    if res.status == 0:        \n",
    "        print(\"No of iterations to optimisation = %d\" % res.nit)\n",
    "        # objective is the sum of y_i.\n",
    "        distance_y = np.sum(res.x[:n])\n",
    "        #print(\"sum y = \", distance_y)\n",
    "        distance_TC = C.flatten().dot(res.x[n:])\n",
    "        #print(\"sum TC = %f\" % distance_TC)\n",
    "        return (distance_y, res.status)\n",
    "    else:\n",
    "        return (0, res.status) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a dataset, predict similarities and save to a file.\n",
    "trans_data = xlrd.open_workbook('../data/newdata.xlsx')  \n",
    "sheet = trans_data.sheet_by_index(0)  \n",
    "scores = []\n",
    "outputs = []\n",
    "distancesst = []\n",
    "distancests = []\n",
    "for l in range(1, sheet.nrows):\n",
    "    rows = sheet.row_values(l, 0, sheet.ncols)\n",
    "    source = list(set(mecab.parse(clean_text(rows[0]).lower().strip('\\n')).split()))\n",
    "    target = list(set(nltk.word_tokenize(clean_text(rows[1]).lower().strip())))\n",
    "    #res1 = sms(source, target)\n",
    "    #distancesst.append(res1)\n",
    "    res1 = mwmd(source, target)\n",
    "    vals_t = -1 if res1[1] > 0 else res1[0]\n",
    "    distancesst.append(vals_t)\n",
    "    \n",
    "    res2 = mwmd(target, source)\n",
    "    valt_s = -1 if res2[1] > 0 else res2[0]\n",
    "    distancests.append(valt_s)\n",
    "\n",
    "distancesst = np.array(distancesst)\n",
    "distancests = np.array(distancests)\n",
    "print(\"stot\",distancesst)\n",
    "print(\"ttos\",distancests)\n",
    "scores = distancesst + distancests\n",
    "\n",
    "\n",
    "scores = np.array(scores)\n",
    "max_val = np.max(scores)\n",
    "print(\"max val\", max_val)\n",
    "scores = 1.0 - (scores / max_val)\n",
    "\n",
    "with open(\"../data/pred-sims-mwmd.csv\", \"w\") as out_file:\n",
    "    for val in scores:\n",
    "        #print(val)\n",
    "        out_file.write(\"%f\\n\" % val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
